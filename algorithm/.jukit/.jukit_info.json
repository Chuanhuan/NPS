{"cmd": "r\"\"\"°°°\n# Variational Inference with Bayesian Multivariate Gaussian Mixture Model\n°°°\"\"\"\n# |%%--%%| <pnyOIiSnDu|4kph9QpG56>\nr\"\"\"°°°\nCoordinate ascent mean-field variational inference (CAVI) using the evidence lower bound (ELBO) to iteratively perform the optimal variational factor distribution parameter updates for clustering.\n°°°\"\"\"\n# |%%--%%| <4kph9QpG56|jwX2O9jJuz>\nr\"\"\"°°°\n## Imports\n°°°\"\"\"\n# |%%--%%| <jwX2O9jJuz|vnuqhRrUSa>\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# |%%--%%| <vnuqhRrUSa|eLAtFeisdQ>\nr\"\"\"°°°\n## Helpers\n°°°\"\"\"\n# |%%--%%| <eLAtFeisdQ|SEVfwFlbtg>\n\ndef generate_data(std, k, n, dim=1):\n    \"\"\" Generates data for mixture of Gaussians.\n    \n    Parameters\n    ----------\n    std : float\n        Standard deviation of normal distributions.\n    k : int\n        Number of clusters.\n    n : int\n        Number of data points.\n    dim : int\n        The dimensionality of the data\n    \n    Returns\n    -------\n    numpy.ndarray\n        The data, of shape (n, dim)\n    categories : list\n        The true category of each data point, of length n.\n    means : numpy.ndarray\n        The true means of each category, of shape (k, dim) \n    \"\"\"\n    # k means of dim dimension\n    means = np.random.normal(0.0, std, size=(k, dim))\n    data = []\n    categories = []\n    \n    for i in range(n):\n        # sample component assignment\n        cat = np.random.choice(k)\n        categories.append(cat)\n        # sample data point from the Gaussian (multivariate)\n        data.append(np.random.multivariate_normal(means[cat, :], np.eye(dim)))\n    \n    return np.stack(data), categories, means\n\n\ndef plot(x, y, c, means, title):\n    plt.scatter(x, y, c=c)\n    plt.scatter(means[:, 0], means[:, 1], c='r')\n    plt.title(title)\n    plt.savefig(f\"{title}.png\")\n    plt.show()\n\n\ndef plot_elbo(elbo):\n    plt.plot(elbo)\n    plt.title('ELBO')\n    plt.savefig(\"ELBO.png\")\n    plt.show()\n\n# |%%--%%| <SEVfwFlbtg|YxoAA38UOq>\nr\"\"\"°°°\n## ELBO\n°°°\"\"\"\n# |%%--%%| <YxoAA38UOq|MlUVDNNoBh>\n\ndef compute_elbo(data, psi, m, s2, sigma2, mu0):\n    \"\"\" Computes the ELBO. The elbo terms are numbered elbo_i where i corresponds to the term number \n    in the report.\n    \n    Parameters\n    ----------\n    data : numpy.ndarray\n        The data, of shape (n, dim)\n    psi : numpy.ndarray\n        Parameters of categorical posterior over mixture assignment.\n        Of shape (n, k).\n        Conjugate prior is Diriclet.\n        np.sum(psi, axis=1) is all ones\n    m : numpy.ndarray\n        Mean parameters of the Gaussian posterior over each component’s mean parameter.\n        Of shape (k, p) where p is the dimesionality of the data.\n        Conjugate prior is Gaussian.\n    s2 : numpy.ndarray\n        Variance (std squared) of the Gaussian posterior over each component’s mean parameter.\n        Of shape (k, 1) since clusters have diagonal covariance matrix, i.e.: all have same variance.\n    sigma2 : numpy.ndarray\n        Variance of the Gaussian of the prior mean per cluster.\n        Of shape (), since it is a float\n    m0 : numpy.ndarray\n        Mean of the Gaussian of the prior mean per cluster.\n        Of shape (p, ) where p is the dimensioanlity of the data.\n        \n    Returns\n    -------\n    elbo : float\n        ELBO.\n    \"\"\"\n    n, p = data.shape\n    k = m.shape[0]\n\n    elbo = 0\n\n    # TODO: compute ELBO\n    # expected log prior over mixture assignments\n    elbo_2 = - n * np.log(k)\n\n    # expected log prior over mixture locations\n    elbo_1 = \\\n        -0.5 * (p * k * np.log(2*np.pi * sigma2))\n    \n    elbo_1_intermediate = 0\n    \n    for k_idx in range(k):\n        elbo_1_intermediate += \\\n            p * s2[k_idx] + np.dot(m[k_idx].T, m[k_idx]) \\\n            - np.dot(m[k_idx].T, mu0) \\\n            - np.dot(mu0.T, m[k_idx]) \\\n            - np.dot(mu0.T, mu0)\n    \n    # -0.5 or +0.5?\n    elbo_1 += -0.5 * sigma2**(-1) * elbo_1_intermediate\n    \n    # expected log likelihood\n    \n    # for diagonal covariance, lambda is set to 1\n    lambda_ = 1.0\n    \n    elbo_3 = 0\n    \n    for n_idx in range(n):\n        for k_idx in range(k):\n            elbo_3 += \\\n                psi[n_idx, k_idx] * \\\n                (\n                    -0.5 * p * np.log(2*np.pi*lambda_**2) \\\n                    -0.5 * lambda_**(-2) * \\\n                    (\n                        np.dot(data[n_idx,:].T, data[n_idx,:]) \\\n                        - np.dot(data[n_idx,:].T, m[k_idx]) \\\n                        - np.dot(m[k_idx].T, data[n_idx,:]) \\\n                        + s2[k_idx] * p \\\n                        + np.dot(m[k_idx].T, m[k_idx])\n                    ) \n                )\n\n    # entropy of variational location posterior\n    \n    elbo_4 = 0\n    \n    for k_idx in range(k):\n        elbo_4 += np.log(2*np.pi) + 2 * np.log(s2[k_idx]) + 1\n\n    # entropy of the variational assignment posterior\n    \n    elbo_5 = 0\n    \n    for n_idx in range(n):\n        for k_idx in range(k):\n            elbo_5 -= psi[n_idx, k_idx] * np.log(psi[n_idx, k_idx])\n    \n    \n    # sum up elbo\n    elbo = elbo_1 + elbo_2 + elbo_3 + elbo_4 + elbo_5\n    \n    return elbo\n\n# |%%--%%| <MlUVDNNoBh|2C73THgltz>\nr\"\"\"°°°\n## CAVI\n°°°\"\"\"\n# |%%--%%| <2C73THgltz|yrivMuNUfe>\n\ndef cavi(data, k, sigma2, m0, eps=1e-15):\n    \"\"\" Coordinate ascent Variational Inference for Bayesian Mixture of Gaussians\n    \n    Parameters\n    ----------\n    data : numpy.ndarray\n        The data, of shape (n, dim)\n    k : int\n        Number of clusters.\n    sigma2 : numpy.ndarray\n        Variance of the Gaussian of the prior mean per cluster.\n        Of shape (), since it is a float\n    m0 : numpy.ndarray\n        Mean of the Gaussian of the prior mean per cluster.\n        Of shape (p, ) where p is the dimensioanlity of the data.\n    eps : float\n        Convergence criterion.\n        \n    Returns\n    -------\n    m : numpy.ndarray\n        Mean parameters of the Gaussian posterior over each component’s mean parameter.\n        Of shape (k, p) where p is the dimesionality of the data.\n        Conjugate prior is Gaussian.\n    s2 : numpy.ndarray\n        Variance (std squared) of the Gaussian posterior over each component’s mean parameter.\n        Of shape (k, 1) since clusters have diagonal covariance matrix, i.e.: all have same variance.\n    psi : numpy.ndarray\n        Parameters of categorical posterior over mixture assignment.\n        Of shape (n, k).\n        Conjugate prior is Diriclet.\n        np.sum(psi, axis=1) is all ones\n    elbo : float\n        ELBO.\n    \"\"\"\n    n, p = data.shape\n    # initialize randomly\n    m = np.random.normal(0., 1., size=(k, p))\n    s2 = np.square(np.random.normal(0., 1., size=(k, 1)))\n    psi = np.random.dirichlet(np.ones(k), size=n)\n\n    # compute ELBO\n    elbo = [compute_elbo(data, psi, m, s2, sigma2, m0)]\n    convergence = 1.\n    # for diagonal covariance, lambda is set to 1\n    lambda_ = 1.0\n    \n    while convergence > eps:  # while ELBO not converged\n        \n        # update categorical\n        for n_idx in range(n):\n            for k_idx in range(k):\n                psi[n_idx, k_idx] = \\\n                    np.exp(\n                        np.dot(data[n_idx].T, m[k_idx]) * lambda_**(-2) \\\n                        - 0.5 * lambda_**(-2) * (np.dot(m[k_idx].T, m[k_idx]) + p*s2[k_idx])\n                    )\n            psi[n_idx] /= np.sum(psi[n_idx])\n        \n        # update posterior parameters for the component means\n        for k_idx in range(k):\n            for n_idx in range(n):\n                s2[k_idx] += lambda_**(-2) * psi[n_idx, k_idx]\n            \n            s2[k_idx] += sigma2**(-2)\n            s2[k_idx] = s2[k_idx]**(-1)\n        \n        for k_idx in range(k):\n            for n_idx in range(n):\n                m[k_idx] += psi[n_idx, k_idx] * data[n_idx]\n            m[k_idx] *= lambda_**(-2)\n            m[k_idx] += m0 / sigma2\n            m[k_idx] *= s2[k_idx]\n        \n        # compute ELBO\n        elbo.append(compute_elbo(data, psi, m, s2, sigma2, m0))\n        convergence = elbo[-1] - elbo[-2]\n\n    return m, s2, psi, elbo\n\n# |%%--%%| <yrivMuNUfe|mG6AJHp7HG>\nr\"\"\"°°°\n## Run\n°°°\"\"\"\n# |%%--%%| <mG6AJHp7HG|c4DinXo4YG>\nr\"\"\"°°°\nNo seed is used to get varying results.\n°°°\"\"\"\n# |%%--%%| <c4DinXo4YG|Et96dAqCBy>\n\n# parameters\np = 2\nk = 5\nsigma = 5.\n\ndata, categories, means = generate_data(std=sigma, k=k, n=500, dim=p)\n\nm = list()\ns2 = list()\npsi = list()\nelbo = list()\nbest_i = 0\n\nfor i in range(10):\n    \n    m_i, s2_i, psi_i, elbo_i = cavi(data, k=k, sigma2=sigma, m0=np.zeros(p))\n    m.append(m_i)\n    s2.append(s2_i)\n    psi.append(psi_i)\n    elbo.append(elbo_i)\n    \n    if i > 0 and elbo[-1][-1] > elbo[best_i][-1]:\n        best_i = i\n\nclass_pred = np.argmax(psi[best_i], axis=1)\nplot(data[:, 0], data[:, 1], categories, means, title='true data')\nplot(data[:, 0], data[:, 1], class_pred, m[best_i], title='posterior')\nplot_elbo(elbo[best_i])\n", "cmd_opts": " -s --md_cell_start=r\\\"\\\"\\\"°°°", "import_complete": 1, "terminal": "nvimterm"}